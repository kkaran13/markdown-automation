name: Update Markdown in All Repositories

on:
  workflow_dispatch:  # manual trigger
  schedule:
    - cron: "0 6 * * 1"  # optional: every Monday 6 AM UTC

jobs:
  markdown:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout current repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: pip install requests

      - name: Run Markdown Updater
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_TOKEN }}  # stored in repo secrets
          TARGET_USER_OR_ORG: kkaran13                 # replace with your username/org
          CONTENT_FILE: sample.md                      # the markdown file to push
          FILENAME: DEPLOYMENT_DOCUMENT.md             # the target filename in repos
          IS_ORG: "false"                               # "true" for org, "false" for personal account
        run: |
          python <<'EOF'
          import os, requests, base64, logging

          logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

          token = os.getenv("GITHUB_TOKEN")
          target = os.getenv("TARGET_USER_OR_ORG")
          content_file = os.getenv("CONTENT_FILE")
          filename = os.getenv("FILENAME")
          is_org = os.getenv("IS_ORG", "true").lower() == "true"

          headers = {
              "Authorization": f"Bearer {token}",
              "Accept": "application/vnd.github.v3+json"
          }
          api = "https://api.github.com"

          # --- Load markdown content ---
          if not os.path.exists(content_file):
              raise FileNotFoundError(f"Markdown source file not found: {content_file}")
          with open(content_file, "r", encoding="utf-8") as f:
              content = f.read()
          content_b64 = base64.b64encode(content.encode()).decode()

          # --- Fetch all repositories ---
          repos = []
          page = 1
          while True:
              if is_org:
                  url = f"{api}/orgs/{target}/repos?per_page=100&page={page}"
              else:
                  url = f"{api}/user/repos?per_page=100&page={page}"
              r = requests.get(url, headers=headers)
              if r.status_code != 200:
                  raise Exception(f"Failed to fetch repos: {r.status_code} {r.text}")
              data = r.json()
              if not data:
                  break
              repos.extend([repo["name"] for repo in data])
              page += 1

          logging.info(f"Found {len(repos)} repositories for {target}")

          # --- Helper: get branches ---
          def get_branches(repo):
              r = requests.get(f"{api}/repos/{target}/{repo}/branches", headers=headers)
              if r.status_code != 200:
                  logging.warning(f"⚠️  Failed to fetch branches for {repo}: {r.status_code}")
                  return []
              return [b["name"] for b in r.json()]

          # --- Helper: update file in branch ---
          def update_file(repo, branch):
              url = f"{api}/repos/{target}/{repo}/contents/{filename}?ref={branch}"
              r = requests.get(url, headers=headers)
              sha = r.json().get("sha") if r.status_code == 200 else None

              data = {
                  "message": f"Update {filename} [Automated]",
                  "content": content_b64,
                  "branch": branch,
              }
              if sha:
                  data["sha"] = sha

              res = requests.put(url, headers=headers, json=data)
              if res.status_code in (200, 201):
                  logging.info(f"✅ {repo}:{branch} updated successfully")
              else:
                  logging.error(f"❌ {repo}:{branch} failed ({res.status_code})")

          # --- Process all repos and branches ---
          for repo in repos:
              branches = get_branches(repo)
              for branch in branches:
                  update_file(repo, branch)
          EOF
